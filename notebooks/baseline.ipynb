{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27ed0ec-ecd3-4e9c-bd6b-d7dd3c0fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501decb-4ee8-441b-98c6-d3077d79c0f4",
   "metadata": {},
   "source": [
    "# utility functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5803f7-c2b9-4e19-baf3-3ca005e95ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME_FIELD = 'Hostname'\n",
    "DATA_PATH = '/opt/notebooks/datasets/NB_GA_Data_1.xlsx'\n",
    "\n",
    "def pull_raw_data(file_location):\n",
    "    raw = pd.read_excel(file_location, None)\n",
    "    return raw\n",
    "\n",
    "def get_region_data(region, file_location):\n",
    "    \"\"\"\n",
    "    Function for fetching region data\n",
    "    :file_location: file path for data source\n",
    "    :region:\n",
    "        NB GA Data 1:\n",
    "            NB | GA | US Data\n",
    "            NB | GA | CA Data\n",
    "            NB | GA | AU Data\n",
    "            NB | GA | NZ Data\n",
    "            NB | GA | JNBO Data\n",
    "        NB GA Data 2:\n",
    "            NB | GA | EU + UK\n",
    "            NB | GA | TW\n",
    "            NB | GA | HK\n",
    "            NB | GA | MY\n",
    "            NB | GA | SG\n",
    "    \"\"\"\n",
    "    all_data = pull_raw_data(file_location)\n",
    "    region_data = all_data[region]\n",
    "    region_data = region_data.copy(deep=False)\n",
    "    region_data['Date'] = pd.to_datetime(region_data['Date'])\n",
    "    region_data.set_index('Date', inplace=True)\n",
    "    region_data.index = pd.DatetimeIndex(region_data.index.values, freq=region_data.index.inferred_freq)\n",
    "    asc = region_data.sort_index()\n",
    "    return asc\n",
    "\n",
    "def get_hostname_data(hostname, region_data):\n",
    "    hostname_data = region_data[region_data[HOSTNAME_FIELD] == hostname]\n",
    "    return hostname_data\n",
    "\n",
    "def apply_index_freq(data, freq):\n",
    "    return data.asfreq(freq)\n",
    "\n",
    "def aggregate_daily_data(data):\n",
    "    data = data.copy(deep=False)\n",
    "    data.loc[:, 'Year'] = data.index.year\n",
    "    data.loc[:, 'Month'] = data.index.month\n",
    "    data.loc[:, 'Day'] = 1\n",
    "    data.loc[:, 'Date'] = pd.to_datetime(data[['Year', 'Month', 'Day']])\n",
    "    return data.groupby('Date').agg({'Sessions':'sum', 'Pageviews':'sum'})\n",
    "\n",
    "def filter_by_date(data, left_datetime=None, right_datetime=None):\n",
    "    \"\"\"\n",
    "    Function for filtering data not recorded by a complete month\n",
    "    :left_datetime: filter data whose date is less than left_datetime\n",
    "    :right_datetime: filter data whose date is greater than right_datetime\n",
    "    \"\"\"\n",
    "    if left_datetime:\n",
    "        data = data[data.index > left_datetime]\n",
    "    \n",
    "    if right_datetime:\n",
    "        data = data[data.index < right_datetime]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5116712-73b8-4a37-b76d-c2f0cbb2eb33",
   "metadata": {},
   "source": [
    "# exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ad8ca5-9347-4766-9fdb-382159fbf665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_smoothing(raw_series, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Function for fitting an exponential smoothing trend to time series data\n",
    "    :param raw_series: a numpy date indexed series with no missing date values\n",
    "    :param alpha: (default 0.05) the smoothing factor (range 0:1) to define the weighting \n",
    "    of prior values to current value's point (lower is smoother)\n",
    "    \"\"\"\n",
    "    output = [raw_series[0]]\n",
    "    for i in range(1, len(raw_series)):\n",
    "        output.append(raw_series[i] * alpha + (1-alpha) * output[i-1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804db50-76f1-44a5-b340-27fb610a6a45",
   "metadata": {},
   "source": [
    "# metric and error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f6bf35a-a047-4611-a582-8c0aabd04d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(raw_series, smoothed_series, window, scale):\n",
    "    \"\"\"\n",
    "    Function for calculating mae through scikit-learn and build stddev error series\n",
    "    :param raw_series: the raw data series, date indexed\n",
    "    :param smoothed_series: the exponentially smoothed series with identical index to raw_series\n",
    "    :param window: the size of the smoothing window\n",
    "    :param scale: percentile value of the standard normal distribution expressed in terms of stddev value\n",
    "    \"\"\"\n",
    "    # dictionary to store the resulting values of the function's logic\n",
    "    res = {}\n",
    "    \n",
    "    mae_value = mean_absolute_error(raw_series[window:], smoothed_series[window:])\n",
    "    \n",
    "    # store the mae value in the dictionary\n",
    "    res['mae'] = mae_value\n",
    "    \n",
    "    # calculate the stddev between the raw data and the smoothed data, filtering out the incomplete lagged\n",
    "    # exponential smoothing data\n",
    "    # the elements of the smoothed series that couldn't calculate based on an incomplete window will be null)\n",
    "    deviation = np.std(raw_series[window:] - smoothed_series[window:])\n",
    "    \n",
    "    # store the stddev data in the dictionary\n",
    "    res['stddev'] = deviation\n",
    "    \n",
    "    # calculate the scaled stddev (e.g. with a scale of '2', we're calculating 2-sigma around the smoothed value)\n",
    "    yhat = mae_value + scale * deviation\n",
    "    \n",
    "    # store the offset values of stddev as two separate series for plotting\n",
    "    res['yhat_low'] = smoothed_series - yhat\n",
    "    res['yhat_high'] = smoothed_series + yhat\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ec468-9352-4409-a5d7-3ed28db3ee99",
   "metadata": {},
   "source": [
    "# smoothing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34fba288-e468-4933-976c-b5b53afd7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_time_plots(time_series, time_series_name, image_name, smoothing_window, exp_alpha=0.05, \n",
    "                        yhat_scale=1.96, style='seaborn-v0_8', plot_size=(16, 24)):\n",
    "    \"\"\" Function for generating the reference exponential smoothing plot for reference\n",
    "    :param time_series: the date indexed time series \n",
    "    :param time_series_name: name of the time series (for plot labeling)\n",
    "    :param image_name: name of the file to save the image as in .svg format\n",
    "    :param smoothing_window: the size of the smoothing window to lag over for exponential smoothing (a bigger window\n",
    "      results in a lower rate of change over time)\n",
    "    :param exp_alpha: smoothing factor (scale 0:1) to define weighting of prior window values on the curve \n",
    "        (higher is less smooth, taking more weight for more recent values in the window)\n",
    "    :param yhat_scale: (default 1.96, representing 97.5% of the standard normal distribution) \n",
    "        factor corresponding to the percentile value fo the standard normal distribution expressed in terms of stddev\n",
    "    :param style: matplotlib.pyplot style type for the plots. defaulted as seaborn style.\n",
    "    :param plot_size: the size of the entire figure being generated in inches.\n",
    "    \"\"\"\n",
    "    # currying dictionary to store the resulting data\n",
    "    reference_collection = {}\n",
    "    # conversion of the series into a pandas Series type\n",
    "    ts = pd.Series(time_series)\n",
    "    # shorthand way of defining an encapsulating formatting type for all pyplot elements within the definition\n",
    "    with plt.style.context(style=style):\n",
    "        # create references to the overall figure element and each of the subplots within the figure (axes)\n",
    "        fig, axes = plt.subplots(3, 1, figsize=plot_size)  \n",
    "        \n",
    "        # cleanup of the plots to allow some spacing for titles / labels\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "        \n",
    "        # create the series for rolling moving average over a specified window (the most basic approach)\n",
    "        moving_avg = ts.rolling(window=smoothing_window).mean()\n",
    "        \n",
    "        # create the exponentially smoothed average series\n",
    "        exp_smoothed = exp_smoothing(ts, exp_alpha)\n",
    "        \n",
    "        # calculate the mae and the error estimations for the moving average using the code from listing 6.2\n",
    "        res = calculate_mae(time_series, moving_avg, smoothing_window, yhat_scale)\n",
    "        \n",
    "        # calculate the mae and error estimations for the exponentially smoothed data\n",
    "        res_exp = calculate_mae(time_series, exp_smoothed, smoothing_window, yhat_scale)\n",
    "        \n",
    "        # create a standard Pandas Series from the exponentially smoothed data\n",
    "        exp_data = pd.Series(exp_smoothed, index=time_series.index)\n",
    "        \n",
    "        # create Pandas Series for the stddev error trends\n",
    "        exp_yhat_low_data = pd.Series(res_exp['yhat_low'], index=time_series.index)\n",
    "        exp_yhat_high_data = pd.Series(res_exp['yhat_high'], index=time_series.index)\n",
    "        \n",
    "        # Plot the raw data\n",
    "        axes[0].plot(ts, '-', label=f'Trend for {time_series_name}')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        axes[0].set_title(f'Raw Data trend for {time_series_name}')\n",
    "        \n",
    "        # plot the moving average data\n",
    "        axes[1].plot(ts, '-', label=f'Trend for {time_series_name}')\n",
    "        axes[1].plot(moving_avg, 'g-', label=f'Moving Average with window: {smoothing_window}')\n",
    "        axes[1].plot(res['yhat_high'], 'r--', label='yhat bounds')\n",
    "        axes[1].plot(res['yhat_low'], 'r--')\n",
    "        axes[1].set_title(f\"Moving Average Trend for window: {smoothing_window} with MAE of: {res['mae']:.1f}\")\n",
    "        axes[1].legend(loc='upper left')\n",
    "        \n",
    "        # plot the exponentially smoothed data\n",
    "        axes[2].plot(ts, '-', label=f'Trend for {time_series_name}')\n",
    "        axes[2].legend(loc='upper left')\n",
    "        axes[2].plot(exp_data, 'g-', label=f'Exponential Smoothing with alpha: {exp_alpha}')\n",
    "        axes[2].plot(exp_yhat_high_data, 'r--', label='yhat bounds')\n",
    "        axes[2].plot(exp_yhat_low_data, 'r--')\n",
    "        axes[2].set_title(f\"Exponential Smoothing Trend for alpha: {exp_alpha} with MAE of: {res_exp['mae']:.1f}\")\n",
    "        axes[2].legend(loc='upper left')\n",
    "        \n",
    "        # save it for reference\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        \n",
    "        # clean up the display to 'make it pretty'\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # record these plots and the series that were calculated from the data in the dictionary\n",
    "        reference_collection['plots'] = fig\n",
    "        reference_collection['moving_average'] = moving_avg\n",
    "        reference_collection['exp_smooth'] = exp_smoothed\n",
    "        \n",
    "        # return the dictionary that we've put the data into\n",
    "        return reference_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585f794-e842-4999-8d3a-1e536edecac8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b4db349-f3a5-4b92-9b6e-91a7043a6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/opt/notebooks/datasets/NB_GA_Data_1.xlsx'\n",
    "\n",
    "newbalancecom = get_hostname_data('www.newbalance.com', \n",
    "                                  region_data = get_region_data('NB | GA | US Data', DATA_PATH))\n",
    "\n",
    "newbalancecom_month = apply_index_freq(aggregate_daily_data(newbalancecom), 'MS')\n",
    "newbalancecom_month = filter_by_date(newbalancecom_month, left_datetime=datetime(2016, 11, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4963fc18-8725-4a1b-a7d4-c8388ef89438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/1453651362.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output = [raw_series[0]]\n",
      "/tmp/ipykernel_3529/1453651362.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output.append(raw_series[i] * alpha + (1-alpha) * output[i-1])\n"
     ]
    }
   ],
   "source": [
    "newbalancecom_reference = smoothed_time_plots(newbalancecom_month['Sessions'], 'newbalance.com Sessions', \n",
    "                                    'newbalancecom_sessions_smooth_plot.svg', 12, exp_alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1ba3e6-0244-44f7-abb4-ca3fe8005e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/1453651362.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output = [raw_series[0]]\n",
      "/tmp/ipykernel_3529/1453651362.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output.append(raw_series[i] * alpha + (1-alpha) * output[i-1])\n"
     ]
    }
   ],
   "source": [
    "newbalancecom_reference = smoothed_time_plots(newbalancecom_month['Sessions'], 'newbalance.com Sessions', \n",
    "                                    'newbalancecom_sessions_smooth_plot.svg', 12, exp_alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
