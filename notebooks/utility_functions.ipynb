{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcf65b0-1113-4254-a0eb-27ad8f79e950",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e0e41-ab36-4718-8a55-4a16eb636fdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## reference:\n",
    "- pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "- pd.DatetimeIndex: https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html\n",
    "- pd.DataFrame.asfreq: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.asfreq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad436917-0225-4377-a498-84d926377ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "HOSTNAME_FIELD = 'Hostname'\n",
    "DATA_PATH = '/opt/notebooks/datasets/NB_GA_Data_1.xlsx'\n",
    "\n",
    "def pull_raw_data(file_location):\n",
    "    raw = pd.read_excel(file_location, None)\n",
    "    return raw\n",
    "\n",
    "def get_region_data(region, file_location):\n",
    "    \"\"\"\n",
    "    Function for fetching region data\n",
    "    :file_location: file path for data source\n",
    "    :region:\n",
    "        NB GA Data 1:\n",
    "            NB | GA | US Data\n",
    "            NB | GA | CA Data\n",
    "            NB | GA | AU Data\n",
    "            NB | GA | NZ Data\n",
    "            NB | GA | JNBO Data\n",
    "        NB GA Data 2:\n",
    "            NB | GA | EU + UK\n",
    "            NB | GA | TW\n",
    "            NB | GA | HK\n",
    "            NB | GA | MY\n",
    "            NB | GA | SG\n",
    "    \"\"\"\n",
    "    all_data = pull_raw_data(file_location)\n",
    "    region_data = all_data[region]\n",
    "    region_data = region_data.copy(deep=False)\n",
    "    region_data['Date'] = pd.to_datetime(region_data['Date'])\n",
    "    region_data.set_index('Date', inplace=True)\n",
    "    region_data.index = pd.DatetimeIndex(region_data.index.values, freq=region_data.index.inferred_freq)\n",
    "    asc = region_data.sort_index()\n",
    "    return asc\n",
    "\n",
    "def describe_region_data(region_data):\n",
    "    return pd.Series({'devices':','.join(list(region_data['Device Category'].unique())),\n",
    "                      'channels':','.join(list(region_data['Default Channel Grouping'].unique())),\n",
    "                      'hostnames':','.join(list(region_data['Hostname'].unique())),\n",
    "                      'start_date':region_data.index.min(),\n",
    "                      'end_date':region_data.index.max()})\n",
    "\n",
    "def get_hostname_data(hostname, region_data):\n",
    "    hostname_data = region_data[region_data[HOSTNAME_FIELD] == hostname]\n",
    "    return hostname_data\n",
    "\n",
    "def apply_index_freq(data, freq):\n",
    "    return data.asfreq(freq)\n",
    "\n",
    "def aggregate_daily_data(data):\n",
    "    data = data.copy(deep=False)\n",
    "    data.loc[:, 'Year'] = data.index.year\n",
    "    data.loc[:, 'Month'] = data.index.month\n",
    "    data.loc[:, 'Day'] = 1\n",
    "    data.loc[:, 'Date'] = pd.to_datetime(data[['Year', 'Month', 'Day']])\n",
    "    return data.groupby('Date').agg({'Sessions':'sum', 'Pageviews':'sum'})\n",
    "\n",
    "def filter_by_date(data, left_datetime=None, right_datetime=None):\n",
    "    \"\"\"\n",
    "    Function for filtering data not recorded by a complete month\n",
    "    :left_datetime: filter data whose date is less than left_datetime\n",
    "    :right_datetime: filter data whose date is greater than right_datetime\n",
    "    \"\"\"\n",
    "    if left_datetime:\n",
    "        data = data[data.index > left_datetime]\n",
    "    \n",
    "    if right_datetime:\n",
    "        data = data[data.index < right_datetime]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def ingest_data(hostname, data_location='NB | GA | US Data', freq='MS', furtherest_date=None, nearest_date=None):\n",
    "    \"\"\"\n",
    "    Function for filtering data not recorded by a complete month\n",
    "    :hostname: filter data by hostname\n",
    "    :data_location: \n",
    "            NB GA Data 1:\n",
    "            NB | GA | US Data\n",
    "            NB | GA | CA Data\n",
    "            NB | GA | AU Data\n",
    "            NB | GA | NZ Data\n",
    "            NB | GA | JNBO Data\n",
    "        NB GA Data 2:\n",
    "            NB | GA | EU + UK\n",
    "            NB | GA | TW\n",
    "            NB | GA | HK\n",
    "            NB | GA | MY\n",
    "            NB | GA | SG\n",
    "    :freq: MS\n",
    "    :left_datetime: filter data whose date is less than left_datetime\n",
    "    :right_datetime: filter data whose date is greater than right_datetime\n",
    "    \"\"\"\n",
    "    data = get_hostname_data(hostname, region_data = get_region_data(data_location, DATA_PATH))\n",
    "    data = apply_index_freq(aggregate_daily_data(data), freq)\n",
    "    data = filter_by_date(data, furtherest_date, nearest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84636afb-0bc5-46a5-b7a5-282385672e75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1299b804-6a5d-4cf9-a8c9-b53a2331082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def generate_outlier_plots(data_series, series_name, series_column, event_date, event_name, image_name):\n",
    "    \"\"\"\n",
    "    Function for generating outlier plot data.\n",
    "    :param data_series: The timeseries series element (indexed by date) to plot\n",
    "    :param series_name: A human-readable name for the time series for applying a label to the plot\n",
    "    :param series_column: The name of the series column within the passed in DataFrame\n",
    "    :event_date: A date specified that marks an outlier event to flag in the plot\n",
    "    :event_name: An explanatory string that represents what the outlier event is to provide context in the plot\n",
    "    :image_name: The name of the file to create in svg format to save the plot to local file system\n",
    "    :return: the pyplot figure.\n",
    "    \"\"\"\n",
    "    # create column names for differencing stationary functions\n",
    "    log_name = f'Log {series_column}'\n",
    "    month_log_name = f'DiffLog {series_column} month'\n",
    "    year_log_name = f'DiffLog {series_column} year'\n",
    "\n",
    "    # convert the passed-in event date to a date time object\n",
    "    event_marker = datetime.strptime(event_date, '%Y-%m-%d').replace(day=1)\n",
    "\n",
    "    # create boundary lines around event for visual purpose\n",
    "    month_delta = relativedelta(months=2)\n",
    "    event_boundary_left = event_marker - month_delta\n",
    "    event_boundary_right = event_marker + month_delta\n",
    "\n",
    "    # get the max value for y-axis so we can draw vertical componet\n",
    "    max_scaling = np.round(data_series[series_column].values.max() * 1.1, 0)\n",
    "\n",
    "    # create a deep copy (in-memory copy of original series)\n",
    "    data = data_series.copy(deep=True)\n",
    "\n",
    "    # add in the differencing columns\n",
    "    data[log_name] = np.log(data[series_column])\n",
    "    data[month_log_name] = data[log_name].diff(1)\n",
    "    data[year_log_name] = data[log_name].diff(12)\n",
    "\n",
    "    # generate the plot layout\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "    axes[0].plot(data[series_column], '-', label=series_column)\n",
    "    axes[0].set(title='{} {}'.format(series_name, series_column))\n",
    "    \n",
    "    # draw the vertical boundary lines (offset 2 months on either side of the passed in date)\n",
    "    axes[0].axvline(event_boundary_left, 0, max_scaling, color='r', linestyle='--', label=event_name)\n",
    "    axes[0].axvline(event_boundary_right, 0, max_scaling, color='r', linestyle='--')\n",
    "    axes[0].legend(loc='best')\n",
    "    \n",
    "    # plot the 1-month differenced series data and overlay the same vertical lines as axes[0] plot.\n",
    "    axes[1].plot(data[month_log_name], label='Monthly diff of {}'.format(series_column))\n",
    "    axes[1].hlines(0, data.index[0], data.index[-1], 'g')\n",
    "    axes[1].set(title='{} Monthly diff of {}'.format(series_name, series_column))\n",
    "    axes[1].axvline(event_boundary_left, 0, max_scaling, color='r', linestyle='--', label=event_name)\n",
    "    axes[1].axvline(event_boundary_right, 0, max_scaling, color='r', linestyle='--')\n",
    "    axes[1].legend(loc='best')\n",
    "    \n",
    "    # plot the 12-month differenced series data.\n",
    "    axes[2].plot(data[year_log_name], label='Year diff of {}'.format(series_column))\n",
    "    axes[2].hlines(0, data.index[0], data.index[-1], 'g')\n",
    "    axes[2].set(title='{} Yearly diff of {}'.format(series_name, series_column))\n",
    "    axes[2].axvline(event_boundary_left, 0, max_scaling, color='r', linestyle='--', label=event_name)\n",
    "    axes[2].axvline(event_boundary_right, 0, max_scaling, color='r', linestyle='--')\n",
    "    axes[2].legend(loc='best')\n",
    "    \n",
    "    # save the image\n",
    "    plt.savefig(image_name, format='svg')\n",
    "    # return the image from the function so that we can embed it in a more complex return type later\n",
    "    # when we want to wrap all of these calls in a more complex execution chain.\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cfaf0-5a92-44ad-a808-1cb42c79ea4a",
   "metadata": {},
   "source": [
    "# Stationary Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f7f82cb-9764-43c1-82be-8fc182488005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def dickey_fuller_test(time_df, series_col):\n",
    "    dickey_fuller_test = adfuller(time_df[series_col].dropna(), autolag='AIC')\n",
    "    test_items = dickey_fuller_test[:4]\n",
    "    report_items = test_items + ((\"not \" if test_items[1] > 0.05 else \"\") + \"stationary\",)\n",
    "    df_report = pd.Series(report_items, index=['Test Statistic', 'p-value', '# Lags', '# Observations', 'Stationary Test'])\n",
    "    for k, v in dickey_fuller_test[4].items():\n",
    "        df_report['Critical Value(%s)' % k] = v\n",
    "    return df_report\n",
    "\n",
    "def plot_diff(time_df, series_col, time_series_name, image_name, style='seaborn-v0_8', plot_size=(16, 12)):\n",
    "    \"\"\" \n",
    "    Function for plotting raw data, 1st-diff and 2nd-diff data\n",
    "    :param time_df: the pandas dataframe with raw values\n",
    "    :param series_col: the col including time series\n",
    "    :param time_series_name: a name for the plot\n",
    "    :param image_name: the name of the file to save the visualization as in svg format\n",
    "    :param style: (default 'seaborn') the visual style of the plots\n",
    "    :param plot_size: (default 16 x 12 inches) the size of the figure we're going to generate\n",
    "    \"\"\"\n",
    "    log_col_name = f'Log {series_col}'\n",
    "    diff1st_log_col_name = f'Log 1stDiff {series_col}'\n",
    "    diff2nd_log_col_name = f'Log 2ndDiff {series_col}'\n",
    "    time_df[log_col_name] = np.log(time_df[series_col])\n",
    "    time_df[diff1st_log_col_name] = time_df[log_col_name].diff()\n",
    "    time_df[diff2nd_log_col_name] = time_df[log_col_name].diff().diff()\n",
    "    df_index_start = time_df.index.values[0]\n",
    "    df_index_end = time_df.index.values[len(time_df)-1]\n",
    "    \n",
    "    # Dickey-Fuller test\n",
    "    # raw data\n",
    "    adfuller_output_raw = dickey_fuller_test(time_df, series_col)\n",
    "    \n",
    "    # create a string to populate a bounding box with on the graph\n",
    "    text_str_raw = '\\n'.join((\n",
    "        'p-value = {:.3f}'.format(adfuller_output_raw['p-value']),\n",
    "        'stationary test = {}'.format(adfuller_output_raw['Stationary Test'])\n",
    "    ))\n",
    "\n",
    "    # 1st diff data\n",
    "    adfuller_output_1stdiff = dickey_fuller_test(time_df, diff1st_log_col_name)\n",
    "    text_str_1stdiff = '\\n'.join((\n",
    "        'p-value = {:.3f}'.format(adfuller_output_1stdiff['p-value']),\n",
    "        'stationary test = {}'.format(adfuller_output_1stdiff['Stationary Test'])\n",
    "    ))\n",
    "\n",
    "    # 2nd diff data\n",
    "    adfuller_output_2nddiff = dickey_fuller_test(time_df, diff2nd_log_col_name)\n",
    "    text_str_2nddiff = '\\n'.join((\n",
    "        'p-value = {:.3f}'.format(adfuller_output_2nddiff['p-value']),\n",
    "        'stationary test = {}'.format(adfuller_output_2nddiff['Stationary Test'])\n",
    "    ))\n",
    "    \n",
    "    with plt.style.context(style=style):\n",
    "        fig, axes = plt.subplots(3, 1, figsize=plot_size)\n",
    "        props = dict(boxstyle='round', facecolor='oldlace', alpha=0.5)\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "        axes[0].plot(time_df[series_col], '-', label=f'Raw data for {time_series_name}')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        axes[0].set_title(f'Raw data trend for {time_series_name}')\n",
    "        axes[0].set_ylabel(series_col)\n",
    "        axes[0].set_xlabel(time_df.index.name)   \n",
    "        axes[0].text(0.05, 0.9, text_str_raw, transform=axes[0].transAxes, fontsize=12, verticalalignment='top', bbox=props)\n",
    "        axes[1].plot(time_df[diff1st_log_col_name], 'g-', label=f'Log 1st Diff for {time_series_name}')\n",
    "        axes[1].hlines(0.0, df_index_start, df_index_end, 'r', label='Series center')\n",
    "        axes[1].legend(loc='lower left')\n",
    "        axes[1].set_title(f'1st Diff Log Trend for {time_series_name}')\n",
    "        axes[1].set_ylabel(series_col)\n",
    "        axes[1].set_xlabel(time_df.index.name) \n",
    "        axes[1].text(0.05, 0.9, text_str_1stdiff, transform=axes[1].transAxes, fontsize=12, verticalalignment='top', bbox=props)\n",
    "        axes[2].plot(time_df[diff2nd_log_col_name], 'b-', label=f'Log 2nd Diff for {time_series_name}')\n",
    "        axes[2].hlines(0.0, df_index_start, df_index_end, 'r', label='Series center')\n",
    "        axes[2].legend(loc='lower left')\n",
    "        axes[2].set_title(f'2nd Diff Log Trend for {time_series_name}')\n",
    "        axes[2].set_ylabel(series_col)\n",
    "        axes[2].set_xlabel(time_df.index.name) \n",
    "        axes[2].text(0.05, 0.9, text_str_2nddiff, transform=axes[2].transAxes, fontsize=12, verticalalignment='top', bbox=props)\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        plt.tight_layout()\n",
    "    return time_df, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4057f864-9cfc-41af-81a9-d40d03250cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def plot_decomposition(time_df, series_col, time_series_name, period, image_name,\n",
    "                       style='seaborn-v0_8', plot_size=(16, 32)):\n",
    "    decomposed_trend = seasonal_decompose(time_df[series_col].dropna(), period=period)\n",
    "    df_index_start = time_df.index.values[0]\n",
    "    df_index_end = time_df.index.values[len(time_df)-1]\n",
    "    with plt.style.context(style=style):\n",
    "        fig, axes = plt.subplots(4, 1, figsize=plot_size)\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "        axes[0].plot(time_df[series_col], '-', label=f'Data for {time_series_name}')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        axes[0].set_title(f'{series_col} trend for {time_series_name}')\n",
    "        axes[0].set_ylabel(series_col)\n",
    "        axes[0].set_xlabel(time_df.index.name)\n",
    "        axes[1].plot(decomposed_trend.trend, 'r-', label=f'Trend data for {time_series_name}')\n",
    "        axes[1].legend(loc='upper left')\n",
    "        axes[1].set_title(f'Trend component of decomposition for {time_series_name} {series_col}')\n",
    "        axes[1].set_ylabel(series_col)\n",
    "        axes[1].set_xlabel(time_df.index.name)\n",
    "        axes[2].plot(decomposed_trend.seasonal, 'r-', label=f'Seasonal data for {time_series_name}')\n",
    "        axes[2].legend(loc='center left', bbox_to_anchor=(0,1))\n",
    "        axes[2].set_title(f'Seasonal component of decomposition for {time_series_name}')\n",
    "        axes[2].set_ylabel(series_col)\n",
    "        axes[2].set_xlabel(time_df.index.name)\n",
    "        axes[3].plot(decomposed_trend.resid, 'r.', label=f'Residuals data for {time_series_name}')\n",
    "        axes[3].hlines(0.0, df_index_start, df_index_end, 'black', label='Series Center')\n",
    "        axes[3].legend(loc='center left', bbox_to_anchor=(0,1))\n",
    "        axes[3].set_title(f'Residuals component of decomposition for {time_series_name}')\n",
    "        axes[3].set_ylabel(series_col)\n",
    "        axes[3].set_xlabel(time_df.index.name)\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "946b6c25-7729-4c8b-bd17-a9ce80ef145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def plot_acf_pacf(time_df, series_col, lags, image_name, style='seaborn-v0_8', plot_size=(16, 32)):\n",
    "    with plt.style.context(style=style):\n",
    "        fig, axes = plt.subplots(2, 1, figsize=plot_size)\n",
    "        fig = plot_acf(time_df[series_col].dropna(), lags=lags, ax=axes[0])\n",
    "        fig = plot_pacf(time_df[series_col].dropna(), lags=lags, ax=axes[1])\n",
    "        axes[0].set_xlabel('lags')\n",
    "        axes[0].set_ylabel('correlation')\n",
    "        axes[1].set_xlabel('lags')\n",
    "        axes[1].set_ylabel('correlation')\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b15b4-0d6e-4229-a4d2-402457157ecd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "481ba47d-63c0-449d-8cd1-47ecfe5dc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "def split_correctness(data, train, test):\n",
    "    \"\"\"\n",
    "    Utility function for making sure that the split that we conducted split the data correctly\n",
    "    :param data: the full data set\n",
    "    :param train: the train portion of the data set\n",
    "    :param test: the test portion of the data set\n",
    "    \"\"\"\n",
    "    assert data.size == train.size + test.size, \\\n",
    "    \"Train count {} and test count {} did not match to source count {}\".format(train.size, test.size, data.size)\n",
    "\n",
    "# parse: https://dateutil.readthedocs.io/en/stable/parser.html\n",
    "def generate_splits(data, date):\n",
    "    \"\"\"\n",
    "    Function for splitting raw data between train and test at a boundary point\n",
    "    that is specified as a parse-able date format.\n",
    "    :param data: the raw data\n",
    "    :param date: a date, in a format that 'can be parsed' to serve as the boundary point\n",
    "    \"\"\"\n",
    "    parsed_date = parse(date, fuzzy=True)\n",
    "    nearest_date = data[:parsed_date].iloc(0)[-1].name\n",
    "    train = data[:nearest_date]\n",
    "    test = data[nearest_date:][1:]\n",
    "    split_correctness(data, train, test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb36b7-e4d4-49d9-ba4e-bc6e8eacbdf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metric and Error Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8d851-6061-4bb1-a81b-0b94a8231e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(raw_series, smoothed_series, window, scale):\n",
    "    \"\"\"\n",
    "    Function for calculating mae through scikit-learn and build stddev error series\n",
    "    :param raw_series: the raw data series, date indexed\n",
    "    :param smoothed_series: the exponentially smoothed series with identical index to raw_series\n",
    "    :param window: the size of the smoothing window\n",
    "    :param scale: percentile value of the standard normal distribution expressed in terms of stddev value\n",
    "    \"\"\"\n",
    "    # dictionary to store the resulting values of the function's logic\n",
    "    res = {}\n",
    "    \n",
    "    mae_value = mean_absolute_error(raw_series[window:], smoothed_series[window:])\n",
    "    \n",
    "    # store the mae value in the dictionary\n",
    "    res['mae'] = mae_value\n",
    "    \n",
    "    # calculate the stddev between the raw data and the smoothed data, filtering out the incomplete lagged\n",
    "    # exponential smoothing data\n",
    "    # the elements of the smoothed series that couldn't calculate based on an incomplete window will be null)\n",
    "    deviation = np.std(raw_series[window:] - smoothed_series[window:])\n",
    "    \n",
    "    # store the stddev data in the dictionary\n",
    "    res['stddev'] = deviation\n",
    "    \n",
    "    # calculate the scaled stddev (e.g. with a scale of '2', we're calculating 2-sigma around the smoothed value)\n",
    "    yhat = mae_value + scale * deviation\n",
    "    \n",
    "    # store the offset values of stddev as two separate series for plotting\n",
    "    res['yhat_low'] = smoothed_series - yhat\n",
    "    res['yhat_high'] = smoothed_series + yhat\n",
    "    return res\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Function for calculating mean absolute percentage error \n",
    "    (for comparing models of different series magnitudes to one another for objective quality comparison)\n",
    "    :param y_true: the 'validation' series (a.k.a. 'test')\n",
    "    :param y_pred: the forecast series (a.k.a. the result of a '.predict()' method call)\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_errors(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Function to calculate the 'core' forecasting error metrics for a regression problem.\n",
    "    :param y_true: the test series (ground truth holdout data)\n",
    "    :param y_pred: the forecast (predicted) series\n",
    "    \"\"\"\n",
    "    # create a dictionary to store all of the metrics\n",
    "    error_scores = {}\n",
    "    \n",
    "    # define a variable for mse\n",
    "    # it's also going to be used to calculate the rmse value\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # store all of the metrics into the dictionary for ease of access later. \n",
    "    #  (don't return tuples - it's a bad practice because it's really hard for other humans to read)\n",
    "    error_scores['mae'] = mean_absolute_error(y_true, y_pred)\n",
    "    error_scores['mape'] = mape(y_true, y_pred)\n",
    "    error_scores['mse'] = mse\n",
    "    error_scores['rmse'] = sqrt(mse)\n",
    "    error_scores['explained_var'] = explained_variance_score(y_true, y_pred)\n",
    "    error_scores['r2'] = r2_score(y_true, y_pred)\n",
    "    return error_scores\n",
    "\n",
    "def plot_predictions(y_true, y_pred, time_series_name, value_name, image_name, style='seaborn-v0_8', plot_size=(16, 12)):\n",
    "    \"\"\" Function for a standardized forecasting visualization\n",
    "    :param y_true: the ground-truth values during the forecasting period\n",
    "    :param y_pred: the forecast values during the holdout period\n",
    "    :param time_series_name: a name for the plot\n",
    "    :param value_name: the name for our y-axis on the plot\n",
    "    :param image_name: the name of the file to save the visualization as in svg format\n",
    "    :param style: (default 'seaborn') the visual style of the plots\n",
    "    :param plot_size: (default 16 x 12 inches) the size of the figure we're going to generate\n",
    "    \"\"\"\n",
    "    # dictionary for currying\n",
    "    validation_output = {} \n",
    "    \n",
    "    # full error metrics suite as shown in listing 6.6\n",
    "    error_values = calculate_errors(y_true, y_pred)\n",
    "    \n",
    "    # store all of the raw values of the errors\n",
    "    validation_output['errors'] = error_values\n",
    "    \n",
    "    # create a string to populate a bounding box with on the graph\n",
    "    text_str = '\\n'.join((\n",
    "        'mae = {:.3f}'.format(error_values['mae']),\n",
    "        'mape = {:.3f}'.format(error_values['mape']),\n",
    "        'mse = {:.3f}'.format(error_values['mse']),\n",
    "        'rmse = {:.3f}'.format(error_values['rmse']),\n",
    "        'explained var = {:.3f}'.format(error_values['explained_var']),\n",
    "        'r squared = {:.3f}'.format(error_values['r2']),\n",
    "    )) \n",
    "    with plt.style.context(style=style):\n",
    "        fig, axes = plt.subplots(1, 1, figsize=plot_size)\n",
    "        axes.plot(y_true, 'b-', label='Test data for {}'.format(time_series_name))\n",
    "        axes.plot(y_pred, 'r-', label='Forecast data for {}'.format(time_series_name))\n",
    "        axes.legend(loc='upper left')\n",
    "        axes.set_title('Raw and Predicted data trend for {}'.format(time_series_name))\n",
    "        axes.set_ylabel(value_name)\n",
    "        axes.set_xlabel(y_true.index.name)\n",
    "\n",
    "         # create an overlay bounding box so that all of our metrics are displayed on the plot\n",
    "        props = dict(boxstyle='round', facecolor='oldlace', alpha=0.5)\n",
    "        axes.text(0.05, 0.9, text_str, transform=axes.transAxes, fontsize=12, verticalalignment='top', bbox=props)\n",
    "        validation_output['plot'] = fig\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        plt.tight_layout()\n",
    "    return validation_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
