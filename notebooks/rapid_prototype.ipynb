{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e27ed0ec-ecd3-4e9c-bd6b-d7dd3c0fd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501decb-4ee8-441b-98c6-d3077d79c0f4",
   "metadata": {},
   "source": [
    "# utility functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5803f7-c2b9-4e19-baf3-3ca005e95ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME_FIELD = 'Hostname'\n",
    "DATA_PATH = '/opt/notebooks/datasets/NB_GA_Data_1.xlsx'\n",
    "\n",
    "def pull_raw_data(file_location):\n",
    "    raw = pd.read_excel(file_location, None)\n",
    "    return raw\n",
    "\n",
    "def get_region_data(region, file_location):\n",
    "    \"\"\"\n",
    "    Function for fetching region data\n",
    "    :file_location: file path for data source\n",
    "    :region:\n",
    "        NB GA Data 1:\n",
    "            NB | GA | US Data\n",
    "            NB | GA | CA Data\n",
    "            NB | GA | AU Data\n",
    "            NB | GA | NZ Data\n",
    "            NB | GA | JNBO Data\n",
    "        NB GA Data 2:\n",
    "            NB | GA | EU + UK\n",
    "            NB | GA | TW\n",
    "            NB | GA | HK\n",
    "            NB | GA | MY\n",
    "            NB | GA | SG\n",
    "    \"\"\"\n",
    "    all_data = pull_raw_data(file_location)\n",
    "    region_data = all_data[region]\n",
    "    region_data = region_data.copy(deep=False)\n",
    "    region_data['Date'] = pd.to_datetime(region_data['Date'])\n",
    "    region_data.set_index('Date', inplace=True)\n",
    "    region_data.index = pd.DatetimeIndex(region_data.index.values, freq=region_data.index.inferred_freq)\n",
    "    asc = region_data.sort_index()\n",
    "    return asc\n",
    "\n",
    "def get_hostname_data(hostname, region_data):\n",
    "    hostname_data = region_data[region_data[HOSTNAME_FIELD] == hostname]\n",
    "    return hostname_data\n",
    "\n",
    "def apply_index_freq(data, freq):\n",
    "    return data.asfreq(freq)\n",
    "\n",
    "def aggregate_daily_data(data):\n",
    "    data = data.copy(deep=False)\n",
    "    data.loc[:, 'Year'] = data.index.year\n",
    "    data.loc[:, 'Month'] = data.index.month\n",
    "    data.loc[:, 'Day'] = 1\n",
    "    data.loc[:, 'Date'] = pd.to_datetime(data[['Year', 'Month', 'Day']])\n",
    "    return data.groupby('Date').agg({'Sessions':'sum', 'Pageviews':'sum'})\n",
    "\n",
    "def filter_by_date(data, left_datetime=None, right_datetime=None):\n",
    "    \"\"\"\n",
    "    Function for filtering data not recorded by a complete month\n",
    "    :left_datetime: filter data whose date is less than left_datetime\n",
    "    :right_datetime: filter data whose date is greater than right_datetime\n",
    "    \"\"\"\n",
    "    if left_datetime:\n",
    "        data = data[data.index > left_datetime]\n",
    "    \n",
    "    if right_datetime:\n",
    "        data = data[data.index < right_datetime]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5116712-73b8-4a37-b76d-c2f0cbb2eb33",
   "metadata": {},
   "source": [
    "# exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ad8ca5-9347-4766-9fdb-382159fbf665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_smoothing(raw_series, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Function for fitting an exponential smoothing trend to time series data\n",
    "    :param raw_series: a numpy date indexed series with no missing date values\n",
    "    :param alpha: (default 0.05) the smoothing factor (range 0:1) to define the weighting \n",
    "    of prior values to current value's point (lower is smoother)\n",
    "    \"\"\"\n",
    "    output = [raw_series[0]]\n",
    "    for i in range(1, len(raw_series)):\n",
    "        output.append(raw_series[i] * alpha + (1-alpha) * output[i-1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804db50-76f1-44a5-b340-27fb610a6a45",
   "metadata": {},
   "source": [
    "# metric and error estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14597c7d-3736-4827-99f6-9ff40efa4d69",
   "metadata": {},
   "source": [
    "## mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6bf35a-a047-4611-a582-8c0aabd04d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(raw_series, smoothed_series, window, scale):\n",
    "    \"\"\"\n",
    "    Function for calculating mae through scikit-learn and build stddev error series\n",
    "    :param raw_series: the raw data series, date indexed\n",
    "    :param smoothed_series: the exponentially smoothed series with identical index to raw_series\n",
    "    :param window: the size of the smoothing window\n",
    "    :param scale: percentile value of the standard normal distribution expressed in terms of stddev value\n",
    "    \"\"\"\n",
    "    # dictionary to store the resulting values of the function's logic\n",
    "    res = {}\n",
    "    \n",
    "    mae_value = mean_absolute_error(raw_series[window:], smoothed_series[window:])\n",
    "    \n",
    "    # store the mae value in the dictionary\n",
    "    res['mae'] = mae_value\n",
    "    \n",
    "    # calculate the stddev between the raw data and the smoothed data, filtering out the incomplete lagged\n",
    "    # exponential smoothing data\n",
    "    # the elements of the smoothed series that couldn't calculate based on an incomplete window will be null)\n",
    "    deviation = np.std(raw_series[window:] - smoothed_series[window:])\n",
    "    \n",
    "    # store the stddev data in the dictionary\n",
    "    res['stddev'] = deviation\n",
    "    \n",
    "    # calculate the scaled stddev (e.g. with a scale of '2', we're calculating 2-sigma around the smoothed value)\n",
    "    yhat = mae_value + scale * deviation\n",
    "    \n",
    "    # store the offset values of stddev as two separate series for plotting\n",
    "    res['yhat_low'] = smoothed_series - yhat\n",
    "    res['yhat_high'] = smoothed_series + yhat\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389a2e1-0b40-421c-955d-f92b914c7d72",
   "metadata": {},
   "source": [
    "## mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f18b768-a5fe-4584-95fe-4ad4050a2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Function for calculating mean absolute percentage error \n",
    "    (for comparing models of different series magnitudes to one another for objective quality comparison)\n",
    "    :param y_true: the 'validation' series (a.k.a. 'test')\n",
    "    :param y_pred: the forecast series (a.k.a. the result of a '.predict()' method call)\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445703d-7c6e-41f4-a62b-76bad8a5738b",
   "metadata": {},
   "source": [
    "## encapsulate all error calculations in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4747ee4e-ca2f-4b31-88fd-2a010cd8f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Function to calculate the 'core' forecasting error metrics for a regression problem.\n",
    "    :param y_true: the test series (ground truth holdout data)\n",
    "    :param y_pred: the forecast (predicted) series\n",
    "    \"\"\"\n",
    "    # create a dictionary to store all of the metrics\n",
    "    error_scores = {}\n",
    "    \n",
    "    # define a variable for mse\n",
    "    # it's also going to be used to calculate the rmse value\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # store all of the metrics into the dictionary for ease of access later. \n",
    "    #  (don't return tuples - it's a bad practice because it's really hard for other humans to read)\n",
    "    error_scores['mae'] = mean_absolute_error(y_true, y_pred)\n",
    "    error_scores['mape'] = mape(y_true, y_pred)\n",
    "    error_scores['mse'] = mse\n",
    "    error_scores['rmse'] = sqrt(mse)\n",
    "    error_scores['explained_var'] = explained_variance_score(y_true, y_pred)\n",
    "    error_scores['r2'] = r2_score(y_true, y_pred)\n",
    "    return error_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ec468-9352-4409-a5d7-3ed28db3ee99",
   "metadata": {},
   "source": [
    "# smoothing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fba288-e468-4933-976c-b5b53afd7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_time_plots(time_series, time_series_name, image_name, smoothing_window, exp_alpha=0.05, \n",
    "                        yhat_scale=1.96, style='seaborn-v0_8', plot_size=(16, 24)):\n",
    "    \"\"\" Function for generating the reference exponential smoothing plot for reference\n",
    "    :param time_series: the date indexed time series \n",
    "    :param time_series_name: name of the time series (for plot labeling)\n",
    "    :param image_name: name of the file to save the image as in .svg format\n",
    "    :param smoothing_window: the size of the smoothing window to lag over for exponential smoothing (a bigger window\n",
    "      results in a lower rate of change over time)\n",
    "    :param exp_alpha: smoothing factor (scale 0:1) to define weighting of prior window values on the curve \n",
    "        (higher is less smooth, taking more weight for more recent values in the window)\n",
    "    :param yhat_scale: (default 1.96, representing 97.5% of the standard normal distribution) \n",
    "        factor corresponding to the percentile value fo the standard normal distribution expressed in terms of stddev\n",
    "    :param style: matplotlib.pyplot style type for the plots. defaulted as seaborn style.\n",
    "    :param plot_size: the size of the entire figure being generated in inches.\n",
    "    \"\"\"\n",
    "    # currying dictionary to store the resulting data\n",
    "    reference_collection = {}\n",
    "    # conversion of the series into a pandas Series type\n",
    "    ts = pd.Series(time_series)\n",
    "    # shorthand way of defining an encapsulating formatting type for all pyplot elements within the definition\n",
    "    with plt.style.context(style=style):\n",
    "        # create references to the overall figure element and each of the subplots within the figure (axes)\n",
    "        fig, axes = plt.subplots(3, 1, figsize=plot_size)  \n",
    "        \n",
    "        # cleanup of the plots to allow some spacing for titles / labels\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "        \n",
    "        # create the series for rolling moving average over a specified window (the most basic approach)\n",
    "        moving_avg = ts.rolling(window=smoothing_window).mean()\n",
    "        \n",
    "        # create the exponentially smoothed average series\n",
    "        exp_smoothed = exp_smoothing(ts, exp_alpha)\n",
    "        \n",
    "        # calculate the mae and the error estimations for the moving average using the code from listing 6.2\n",
    "        res = calculate_mae(time_series, moving_avg, smoothing_window, yhat_scale)\n",
    "        \n",
    "        # calculate the mae and error estimations for the exponentially smoothed data\n",
    "        res_exp = calculate_mae(time_series, exp_smoothed, smoothing_window, yhat_scale)\n",
    "        \n",
    "        # create a standard Pandas Series from the exponentially smoothed data\n",
    "        exp_data = pd.Series(exp_smoothed, index=time_series.index)\n",
    "        \n",
    "        # create Pandas Series for the stddev error trends\n",
    "        exp_yhat_low_data = pd.Series(res_exp['yhat_low'], index=time_series.index)\n",
    "        exp_yhat_high_data = pd.Series(res_exp['yhat_high'], index=time_series.index)\n",
    "        \n",
    "        # Plot the raw data\n",
    "        axes[0].plot(ts, '-', label=f'Trend for {time_series_name}')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        axes[0].set_title(f'Raw Data trend for {time_series_name}')\n",
    "        \n",
    "        # plot the moving average data\n",
    "        axes[1].plot(ts, '-', label=f'Trend for {time_series_name}')\n",
    "        axes[1].plot(moving_avg, 'g-', label=f'Moving Average with window: {smoothing_window}')\n",
    "        axes[1].plot(res['yhat_high'], 'r--', label='yhat bounds')\n",
    "        axes[1].plot(res['yhat_low'], 'r--')\n",
    "        axes[1].set_title(f\"Moving Average Trend for window: {smoothing_window} with MAE of: {res['mae']:.1f}\")\n",
    "        axes[1].legend(loc='upper left')\n",
    "        \n",
    "        # plot the exponentially smoothed data\n",
    "        axes[2].plot(ts, '-', label=f'Trend for {time_series_name}')\n",
    "        axes[2].legend(loc='upper left')\n",
    "        axes[2].plot(exp_data, 'g-', label=f'Exponential Smoothing with alpha: {exp_alpha}')\n",
    "        axes[2].plot(exp_yhat_high_data, 'r--', label='yhat bounds')\n",
    "        axes[2].plot(exp_yhat_low_data, 'r--')\n",
    "        axes[2].set_title(f\"Exponential Smoothing Trend for alpha: {exp_alpha} with MAE of: {res_exp['mae']:.1f}\")\n",
    "        axes[2].legend(loc='upper left')\n",
    "        \n",
    "        # save it for reference\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        \n",
    "        # clean up the display to 'make it pretty'\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # record these plots and the series that were calculated from the data in the dictionary\n",
    "        reference_collection['plots'] = fig\n",
    "        reference_collection['moving_average'] = moving_avg\n",
    "        reference_collection['exp_smooth'] = exp_smoothed\n",
    "        \n",
    "        # return the dictionary that we've put the data into\n",
    "        return reference_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba77b118-5108-4d05-b2f6-08e6c6268167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred, time_series_name, value_name, image_name, style='seaborn-v0_8', plot_size=(16, 12)):\n",
    "    \"\"\" Function for a standardized forecasting visualization\n",
    "    :param y_true: the ground-truth values during the forecasting period\n",
    "    :param y_pred: the forecast values during the holdout period\n",
    "    :param time_series_name: a name for the plot\n",
    "    :param value_name: the name for our y-axis on the plot\n",
    "    :param image_name: the name of the file to save the visualization as in svg format\n",
    "    :param style: (default 'seaborn') the visual style of the plots\n",
    "    :param plot_size: (default 16 x 12 inches) the size of the figure we're going to generate\n",
    "    \"\"\"\n",
    "    # dictionary for currying\n",
    "    validation_output = {} \n",
    "    \n",
    "    # full error metrics suite as shown in listing 6.6\n",
    "    error_values = calculate_errors(y_true, y_pred)\n",
    "    \n",
    "    # store all of the raw values of the errors\n",
    "    validation_output['errors'] = error_values\n",
    "    \n",
    "    # create a string to populate a bounding box with on the graph\n",
    "    text_str = '\\n'.join((\n",
    "        'mae = {:.3f}'.format(error_values['mae']),\n",
    "        'mape = {:.3f}'.format(error_values['mape']),\n",
    "        'mse = {:.3f}'.format(error_values['mse']),\n",
    "        'rmse = {:.3f}'.format(error_values['rmse']),\n",
    "        'explained var = {:.3f}'.format(error_values['explained_var']),\n",
    "        'r squared = {:.3f}'.format(error_values['r2']),\n",
    "    )) \n",
    "    with plt.style.context(style=style):\n",
    "        fig, axes = plt.subplots(1, 1, figsize=plot_size)\n",
    "        axes.plot(y_true, 'b-', label='Test data for {}'.format(time_series_name))\n",
    "        axes.plot(y_pred, 'r-', label='Forecast data for {}'.format(time_series_name))\n",
    "        axes.legend(loc='upper left')\n",
    "        axes.set_title('Raw and Predicted data trend for {}'.format(time_series_name))\n",
    "        axes.set_ylabel(value_name)\n",
    "        axes.set_xlabel(y_true.index.name)\n",
    "\n",
    "         # create an overlay bounding box so that all of our metrics are displayed on the plot\n",
    "        props = dict(boxstyle='round', facecolor='oldlace', alpha=0.5)\n",
    "        axes.text(0.05, 0.9, text_str, transform=axes.transAxes, fontsize=12, verticalalignment='top', bbox=props)\n",
    "        validation_output['plot'] = fig\n",
    "        plt.savefig(image_name, format='svg')\n",
    "        plt.tight_layout()\n",
    "    return validation_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9fbf76-0be9-444e-a37c-01a8449b39ca",
   "metadata": {},
   "source": [
    "# Time split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7672c91a-7468-4436-b424-d07610727653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_correctness(data, train, test):\n",
    "    \"\"\"\n",
    "    Utility function for making sure that the split that we conducted split the data correctly\n",
    "    :param data: the full data set\n",
    "    :param train: the train portion of the data set\n",
    "    :param test: the test portion of the data set\n",
    "    \"\"\"\n",
    "    assert data.size == train.size + test.size, \\\n",
    "    \"Train count {} and test count {} did not match to source count {}\".format(train.size, test.size, data.size)\n",
    "\n",
    "# parse: https://dateutil.readthedocs.io/en/stable/parser.html\n",
    "def generate_splits(data, date):\n",
    "    \"\"\"\n",
    "    Function for splitting raw data between train and test at a boundary point\n",
    "    that is specified as a parse-able date format.\n",
    "    :param data: the raw data\n",
    "    :param date: a date, in a format that 'can be parsed' to serve as the boundary point\n",
    "    \"\"\"\n",
    "    parsed_date = parse(date, fuzzy=True)\n",
    "    nearest_date = data[:parsed_date].iloc(0)[-1].name\n",
    "    train = data[:nearest_date]\n",
    "    test = data[nearest_date:][1:]\n",
    "    split_correctness(data, train, test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585f794-e842-4999-8d3a-1e536edecac8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e678f-f530-489e-92cd-f0e384d1bbb9",
   "metadata": {},
   "source": [
    "## data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4db349-f3a5-4b92-9b6e-91a7043a6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/opt/notebooks/datasets/NB_GA_Data_1.xlsx'\n",
    "\n",
    "newbalancecom = get_hostname_data('www.newbalance.com', \n",
    "                                  region_data = get_region_data('NB | GA | US Data', DATA_PATH))\n",
    "\n",
    "newbalancecom_month = apply_index_freq(aggregate_daily_data(newbalancecom), 'MS')\n",
    "newbalancecom_month = filter_by_date(newbalancecom_month, left_datetime=datetime(2016, 11, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16579f9-f074-4c7a-9637-2f1b84f682e5",
   "metadata": {},
   "source": [
    "## time split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd022f9-62d1-4cd4-b16f-1586e5a942ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sessions</th>\n",
       "      <th>Pageviews</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>7255192</td>\n",
       "      <td>28948719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-01</th>\n",
       "      <td>6645071</td>\n",
       "      <td>26869495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sessions  Pageviews\n",
       "Date                           \n",
       "2021-03-01   7255192   28948719\n",
       "2021-04-01   6645071   26869495"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newbalancecom_train_check, newbalancecom_test_check = generate_splits(newbalancecom_month, \"2021-04-13\")\n",
    "newbalancecom_train_check.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abc398-911f-4269-a3f5-a8df62ef806e",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4963fc18-8725-4a1b-a7d4-c8388ef89438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/1453651362.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output = [raw_series[0]]\n",
      "/tmp/ipykernel_3529/1453651362.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output.append(raw_series[i] * alpha + (1-alpha) * output[i-1])\n"
     ]
    }
   ],
   "source": [
    "newbalancecom_reference = smoothed_time_plots(newbalancecom_month['Sessions'], 'newbalance.com Sessions', \n",
    "                                    'newbalancecom_sessions_smooth_plot.svg', 12, exp_alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1ba3e6-0244-44f7-abb4-ca3fe8005e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3529/1453651362.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output = [raw_series[0]]\n",
      "/tmp/ipykernel_3529/1453651362.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output.append(raw_series[i] * alpha + (1-alpha) * output[i-1])\n"
     ]
    }
   ],
   "source": [
    "newbalancecom_reference = smoothed_time_plots(newbalancecom_month['Sessions'], 'newbalance.com Sessions', \n",
    "                                    'newbalancecom_sessions_smooth_plot.svg', 12, exp_alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26f1aa-2585-41da-96f2-271cfa0f42d0",
   "metadata": {},
   "source": [
    "## var model\n",
    "- usage example from api doc: https://www.statsmodels.org/dev/vector_ar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d985f3b8-e98e-441c-80d0-0d64ce10fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = generate_splits(newbalancecom_month, '2021-12-01')\n",
    "var_model = VAR(train[['Sessions', 'Pageviews']])\n",
    "var_model.select_order(12)\n",
    "var_fit = var_model.fit()\n",
    "lag_order = var_fit.k_ar\n",
    "var_pred = var_fit.forecast(test[['Sessions', 'Pageviews']].values[-lag_order:], \n",
    "                            test.index.size)\n",
    "\n",
    "# To plot all of this, we'll need to extract the predictions from the forecast data. Geez this seems complex...\n",
    "var_pred_session = pd.Series(np.asarray(list(zip(*var_pred))[0], dtype=np.float32), index=test.index)\n",
    "var_pred_pageview = pd.Series(np.asarray(list(zip(*var_pred))[1], dtype=np.float32), index=test.index)\n",
    "\n",
    "# Let's use our plot_predictions() function to see how we did!\n",
    "var_prediction_score = plot_predictions(test['Sessions'], \n",
    "                                        var_pred_session, \n",
    "                                        \"VAR model Sessions newbalance.com\", \n",
    "                                        \"Sessions\", \n",
    "                                        \"var_newbalancecom_sessions.svg\")\n",
    "\n",
    "var_prediction_score_intl = plot_predictions(test['Pageviews'], \n",
    "                                        var_pred_pageview, \n",
    "                                        \"VAR model Pageviews newbalance.com\", \n",
    "                                        \"Pageviews\", \n",
    "                                        \"var_newbalancecom_pageview.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f962c5f-8297-4377-9582-0a758a13568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_model = VAR(train[['Sessions', 'Pageviews']])\n",
    "var_model.select_order(12)\n",
    "var_fit = var_model.fit(12)\n",
    "lag_order = var_fit.k_ar\n",
    "var_pred = var_fit.forecast(test[['Sessions', 'Pageviews']].values[-lag_order:], \n",
    "                            test.index.size)\n",
    "\n",
    "# To plot all of this, we'll need to extract the predictions from the forecast data. Geez this seems complex...\n",
    "var_pred_session = pd.Series(np.asarray(list(zip(*var_pred))[0], dtype=np.float32), index=test.index)\n",
    "var_pred_pageview = pd.Series(np.asarray(list(zip(*var_pred))[1], dtype=np.float32), index=test.index)\n",
    "\n",
    "# Let's use our plot_predictions() function to see how we did!\n",
    "var_prediction_score = plot_predictions(test['Sessions'], \n",
    "                                        var_pred_session, \n",
    "                                        \"VAR model Sessions newbalance.com\", \n",
    "                                        \"Sessions\", \n",
    "                                        \"var_newbalancecom_sessions_lag12.svg\")\n",
    "\n",
    "var_prediction_score_intl = plot_predictions(test['Pageviews'], \n",
    "                                        var_pred_pageview, \n",
    "                                        \"VAR model Pageviews newbalance.com\", \n",
    "                                        \"Pageviews\", \n",
    "                                        \"var_newbalancecom_pageview_lag12.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98e312f2-d8d3-4c90-85f7-f7a75d3d89db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_638/1759953441.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  var_pred_session_expanded = np.exp(var_pred_session.cumsum()) * test['Sessions'][0]\n",
      "/tmp/ipykernel_638/1759953441.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  var_pred_pageview_expanded = np.exp(var_pred_pageview.cumsum()) * test['Pageviews'][0]\n"
     ]
    }
   ],
   "source": [
    "# Let's take the log of the series, then do a basic differencing function on the data\n",
    "newbalancecom_month['Sessions Diff'] = np.log(newbalancecom_month['Sessions']).diff()\n",
    "newbalancecom_month['Pageviews Diff'] = np.log(newbalancecom_month['Pageviews']).diff()\n",
    "\n",
    "# since we're differencing, we need to drop the leading value from the series as it is null.\n",
    "newbalancecom_month = newbalancecom_month.dropna()\n",
    "train, test = generate_splits(newbalancecom_month, '2021-12-01')\n",
    "\n",
    "var_model = VAR(train[['Sessions Diff', 'Pageviews Diff']])\n",
    "var_model.select_order(12)\n",
    "var_fit = var_model.fit(12)\n",
    "lag_order = var_fit.k_ar\n",
    "var_pred = var_fit.forecast(test[['Sessions Diff', 'Pageviews Diff']].values[-lag_order:], \n",
    "                            test.index.size)\n",
    "var_pred_session = pd.Series(np.asarray(list(zip(*var_pred))[0], dtype=np.float32), index=test.index)\n",
    "var_pred_pageview = pd.Series(np.asarray(list(zip(*var_pred))[1], dtype=np.float32), index=test.index)\n",
    "\n",
    "# After modeling is complete and the forecast is generated, we need to remove the differencing aspect and then \n",
    "# invert the log values to get back to the original numeric space.\n",
    "var_pred_session_expanded = np.exp(var_pred_session.cumsum()) * test['Sessions'][0]\n",
    "var_pred_pageview_expanded = np.exp(var_pred_pageview.cumsum()) * test['Pageviews'][0]\n",
    "\n",
    "# Let's use our plot_predictions() function to see how we did!\n",
    "var_prediction_score = plot_predictions(test['Sessions'], \n",
    "                                        var_pred_session_expanded, \n",
    "                                        \"VAR model Sessions newbalance.com Diff\", \n",
    "                                        \"Sessions Diff\", \n",
    "                                        \"var_newbalancecom_sessions_lag12_diff.svg\")\n",
    "\n",
    "var_prediction_score_intl = plot_predictions(test['Pageviews'], \n",
    "                                             var_pred_pageview_expanded, \n",
    "                                             \"VAR model Pageviews newbalance.com Diff\", \n",
    "                                             \"Pageviews Diff\", \n",
    "                                             \"var_newbalancecom_pageview_lag12_diff.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af01f65-4af1-4156-a3f1-da58f6d613f1",
   "metadata": {},
   "source": [
    "## Linear model: linear regression, RidgeCV, LassoCV, ElasticNetCV, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0de2fdd-e316-45df-90ab-daf66483814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ingestion\n",
    "DATA_PATH = '/opt/notebooks/datasets/NB_GA_Data_1.xlsx'\n",
    "newbalancecom = get_hostname_data('www.newbalance.com', \n",
    "                                  region_data = get_region_data('NB | GA | US Data', DATA_PATH))\n",
    "newbalancecom_month = apply_index_freq(aggregate_daily_data(newbalancecom), 'MS')\n",
    "newbalancecom_month = filter_by_date(newbalancecom_month, left_datetime=datetime(2016, 11, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf18acb3-7fd5-4ed2-838b-42ed11929f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_638/2457782910.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  newbalancecom_month['years_from_start'] = (newbalancecom_month['Year'] - newbalancecom_month['Year'][0])\n"
     ]
    }
   ],
   "source": [
    "# generate features\n",
    "newbalancecom_month['Year'] = newbalancecom_month.index.year\n",
    "newbalancecom_month['Month'] = newbalancecom_month.index.month\n",
    "newbalancecom_month['months_from_start'] = ((newbalancecom_month.index.year - newbalancecom_month.index[0].year) * 12) + (newbalancecom_month.index.month - newbalancecom_month.index[0].month)\n",
    "newbalancecom_month['years_from_start'] = (newbalancecom_month['Year'] - newbalancecom_month['Year'][0])\n",
    "newbalancecom_month['thanksgiving'] = (newbalancecom_month['Month'] == 11) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cfc58c78-6d1d-4dc2-beab-042cc05aa6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64090504.0, tolerance: 16351759.91912139\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 59762985.78125, tolerance: 16351759.91912139\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51013148.65625, tolerance: 16351759.91912139\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43467806.75, tolerance: 16351759.91912139\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 36965963.75, tolerance: 16351759.91912139\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31367613.21875, tolerance: 16351759.91912139\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16038329.25, tolerance: 14876722.32240393\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "X = newbalancecom_month[['months_from_start', 'years_from_start', 'thanksgiving']]\n",
    "Y = newbalancecom_month[['Sessions']]\n",
    "\n",
    "date_cutoff = '2021-12-01'\n",
    "X_train, X_test = generate_splits(X, date_cutoff)\n",
    "Y_train, Y_test = generate_splits(Y, date_cutoff)\n",
    "Y_train_arr = Y_train.values.ravel()\n",
    "Y_test_arr = Y_test.values.ravel()\n",
    "\n",
    "lr = LinearRegression(\n",
    "    fit_intercept=False\n",
    ").fit(X_train, Y_train_arr)\n",
    "\n",
    "lr_validate = Y_test.copy(deep=False)\n",
    "lr_validate['prediction'] = lr.predict(X_test)\n",
    "\n",
    "# ridge regression\n",
    "ridge = RidgeCV(\n",
    "    alphas=[0.1, 0.5, 0.7, 0.95, 1.0, 10.0],\n",
    "    fit_intercept=False,\n",
    "    gcv_mode='auto' # 'svd', 'eigen'\n",
    ").fit(X_train, Y_train_arr)\n",
    "\n",
    "ridge_validate = Y_test.copy(deep=False)\n",
    "ridge_validate['prediction'] = ridge.predict(X_test)\n",
    "\n",
    "# lasso regression\n",
    "lasso = LassoCV(\n",
    "    eps=1e-3,\n",
    "    n_alphas=100,\n",
    "    fit_intercept=False,\n",
    "    precompute='auto',\n",
    "    tol=1e-8,\n",
    "    selection='cyclic', #cyclic\n",
    "    random_state=42\n",
    ").fit(X_train, Y_train_arr)\n",
    "\n",
    "lasso_validate = Y_test.copy(deep=False)\n",
    "lasso_validate['prediction'] = lasso.predict(X_test)\n",
    "\n",
    "# elastic net\n",
    "elastic = ElasticNetCV(\n",
    "    l1_ratio=[1e-6, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1.0],\n",
    "    n_alphas=100,\n",
    "    fit_intercept=False,\n",
    "    precompute='auto',\n",
    "    tol=1e-4,\n",
    "    positive=False,\n",
    "    selection='random', #cyclic\n",
    "    random_state=42\n",
    ").fit(X_train, Y_train_arr)\n",
    "\n",
    "elastic_validate = Y_test.copy(deep=False)\n",
    "elastic_validate['prediction'] = elastic.predict(X_test)\n",
    "\n",
    "# ensemble\n",
    "ensemble = Y_test.copy(deep=False)\n",
    "ensemble['ols'] = lr.predict(X_test)\n",
    "ensemble['ridge'] = ridge.predict(X_test)\n",
    "ensemble['lasso'] = lasso.predict(X_test)\n",
    "ensemble['elastic'] = lasso.predict(X_test)\n",
    "ensemble['prediction'] = ensemble[['ols', 'ridge', 'lasso', 'elastic']].mean(axis=1)\n",
    "\n",
    "# Let's see what OLS regression can do for us...\n",
    "ols_plot = plot_predictions(Y_test['Sessions'], lr_validate['prediction'], \n",
    "                 'OLS Regression newbalancecom Sessions','Sessions', 'ols_reg.svg')\n",
    "# Check ridge regression\n",
    "ridge_plot = plot_predictions(Y_test['Sessions'], ridge_validate['prediction'], \n",
    "                 'Ridge Regression newbalancecom Sessions', 'Sessions', 'ridge.svg')\n",
    "# and Lasso\n",
    "lasso_plot = plot_predictions(Y_test['Sessions'], lasso_validate['prediction'], \n",
    "                 'Lasso Regression newbalancecom Sessions', 'Sessions', 'lasso.svg')\n",
    "# and ElasticNet\n",
    "enet_plot = plot_predictions(Y_test['Sessions'], elastic_validate['prediction'], \n",
    "                 'ElasticNet Regression newbalancecom Sessions', 'Sessions', 'enet.svg')\n",
    "# see what the ensemble looks like\n",
    "ensemble_plot = plot_predictions(Y_test['Sessions'], ensemble['prediction'], \n",
    "                 'Ensemble Regression newbalancecom Sessions', 'Sessions', 'ensemble.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60731a-839e-4055-af26-156111bd90e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed09cea-6b15-4604-957e-1aa76e60543c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
